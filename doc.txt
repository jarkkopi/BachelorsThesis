Version 1-

Audio - using PANNS-inference audiotagging and labels.

import os
import librosa
import numpy as np
import json
import spacy
from panns_inference import AudioTagging, labels

audio_folder = './data/test_audio_output'

at = AudioTagging(checkpoint_path=None, device='cuda')

sr = 32000 
clip_duration = 5
overlap = 2
threshold = 0.5

# process audio and extract tags
def process_audio(audio_path):
    audio, _ = librosa.load(audio_path, sr=sr, mono=True)

    segment_length = int(clip_duration * sr)
    hop_length = int((clip_duration - overlap) * sr)

    # tags for each segment
    tags_list = []
    for start_idx in range(0, len(audio) - segment_length + 1, hop_length):
        segment = audio[start_idx : start_idx + segment_length][None, :]  # (batch_size, segment_samples)

        clipwise_output, _ = at.inference(segment)
        tags = [labels[i] for i, score in enumerate(clipwise_output[0]) if score > threshold]

        tags_list.append(tags)

    return tags_list

# first 10 audio clips
all_tags = []
processed_clips = 0
for file_name in sorted(os.listdir(audio_folder)):
    if processed_clips >= 10:
        break

    if file_name.endswith('.wav'):
        audio_path = os.path.join(audio_folder, file_name)
        tags = process_audio(audio_path)
        all_tags.append(tags)
        processed_clips += 1

# results
for i, clip_tags in enumerate(all_tags):
    print(f"Audio Clip {i + 1} Tags:")
    for j, segment_tags in enumerate(clip_tags):
        print(f"  Segment {j + 1}: {segment_tags}")
    print()

Captions - using spacy to gather nouns and verbs

import json
import spacy

json_file_path = './data/test_captions.json'
nlp = spacy.load('en_core_web_sm')

with open(json_file_path, 'r') as f:
    captions_data = json.load(f)

def extract_nouns_and_verbs(captions_data):
    connected_words = {}

    for key, captions in list(captions_data.items())[:10]:  # first ten
        all_captions = captions.get("audio_captions", []) + captions.get("visual_captions", []) + captions.get("audio_visual_captions", []) + captions.get("GPT_AV_captions", [])
        connected_words[key] = {"nouns": [], "verbs": []}

        for caption in all_captions:
            doc = nlp(caption)
            """
            for token in doc:
                if token.pos_ in {"NOUN", "VERB"} and token.head.pos_ in {"NOUN", "VERB"}:
                    connected_words[key].append((token.text, token.head.text))
            """
            for token in doc:
                if token.pos_ == "NOUN":
                    connected_words[key]["nouns"].append(token.text)
                elif token.pos_ == "VERB":
                    connected_words[key]["verbs"].append(token.text)
    return connected_words

connected_words = extract_nouns_and_verbs(captions_data)

for key, words in connected_words.items():
    print(f"Key: {key}")
    """
    print(f"Connected Nouns and Verbs: {words}")
    """
    print(f"Nouns: {words['nouns']}")
    print(f"Verbs: {words['verbs']}")


Version 2-

Audio, captions, embeddings and SBERT

import os
import librosa
import numpy as np
import json
import spacy
from panns_inference import AudioTagging, labels
from sentence_transformers import SentenceTransformer, util

audio_folder = './data/test_audio_output'
json_file_path = './data/test_captions.json'

# parameters
num_clips = 10  # Number of audio clips to process
sr = 32000  # sr
clip_duration = 5  # audio segment length
overlap = 2  # overlap in audio (s)
threshold = 0.5  # confidence threshold
weight_audio_conf = 0.6  # audio confidence weight
weight_text_sim = 0.4  # text similarity weight

# Load models
nlp = spacy.load('en_core_web_sm')
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')
at = AudioTagging(checkpoint_path=None, device='cuda')

# Process audio and extract tags
def process_audio(audio_path):
    audio, _ = librosa.load(audio_path, sr=sr, mono=True)
    segment_length = int(clip_duration * sr)
    hop_length = int((clip_duration - overlap) * sr)

    tags_list = []
    for start_idx in range(0, len(audio) - segment_length + 1, hop_length):
        segment = audio[start_idx : start_idx + segment_length][None, :]
        clipwise_output, _ = at.inference(segment)
        tags = [(labels[i], score) for i, score in enumerate(clipwise_output[0]) if score > threshold]
        tags_list.append(tags)
    
    return tags_list

def extract_nouns_and_verbs(captions_data):
    connected_words = {}
    for key, captions in list(captions_data.items())[:num_clips]:
        all_captions = captions.get("audio_captions", []) 
        connected_words[key] = {"nouns": [], "verbs": []}
        
        for caption in all_captions:
            doc = nlp(caption)
            for token in doc:
                if token.pos_ == "NOUN":
                    connected_words[key]["nouns"].append(token.text)
                elif token.pos_ == "VERB":
                    connected_words[key]["verbs"].append(token.text)
    return connected_words

# similarity audio tags and captions
def compute_similarity(audio_tags, caption_words):
    if not caption_words:
        return [(tag, conf * weight_audio_conf) for tag, conf in audio_tags]  # Reduce confidence if no matching words
    
    if not audio_tags:
        return [] 

    audio_labels = [tag for tag, _ in audio_tags]
    audio_embeddings = sbert_model.encode(audio_labels)
    text_embeddings = sbert_model.encode(caption_words) if caption_words else np.array([])
    
    if text_embeddings.shape[0] == 0:  # If no valid text embeddings
        return [(tag, conf * weight_audio_conf) for tag, conf in audio_tags]

    similarity_matrix = util.cos_sim(audio_embeddings, text_embeddings).numpy()
    
    confidence_scores = []
    for i, (tag, conf) in enumerate(audio_tags):
        max_sim = np.max(similarity_matrix[i]) if similarity_matrix.shape[1] > 0 else 0
        final_conf = weight_audio_conf * conf + weight_text_sim * max_sim
        confidence_scores.append((tag, final_conf))
    
    return confidence_scores

if __name__ == "__main__":
    with open(json_file_path, 'r') as f:
        captions_data = json.load(f)

    all_tags = []
    processed_clips = 0
    for file_name in sorted(os.listdir(audio_folder)):
        if processed_clips >= num_clips:
            break
        if file_name.endswith('.wav'):
            audio_path = os.path.join(audio_folder, file_name)
            tags = process_audio(audio_path)
            all_tags.append(tags)
            processed_clips += 1

    connected_words = extract_nouns_and_verbs(captions_data)

    for i, clip_tags in enumerate(all_tags):
        key = list(connected_words.keys())[i] if i < len(connected_words) else None
        if key:
            caption_words = connected_words[key]['nouns'] + connected_words[key]['verbs']
            for j, segment_tags in enumerate(clip_tags):
                final_scores = compute_similarity(segment_tags, caption_words)
                print(f"Audio Clip {i + 1}, Segment {j + 1} Confidence Scores:")
                for tag, score in final_scores:
                    print(f"  Tag: {tag}, Confidence: {score:.2f}")
                print()

Version 3- printing tags in both:

Add this code to compute_similarity:

print("Audio Tags (Words):", audio_labels)
print("Text (Words):", caption_words)


Version 4-
import os
import librosa
import numpy as np
import json
import spacy
from panns_inference import AudioTagging, labels
from sentence_transformers import SentenceTransformer, util

audio_folder = './data/test_audio_output'
json_file_path = './data/test_captions.json'

# Parameters
num_clips = 5  # Number of audio clips to process
sr = 32000  # Sample rate
clip_duration = 5  # Audio segment length
overlap = 2  # Overlap in audio (s)
threshold = 0.5  # Confidence threshold
weight_audio_conf = 0.4  # Audio confidence weight
weight_text_sim = 0.6  # Text similarity weight

# Load models
nlp = spacy.load('en_core_web_sm')
sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
at = AudioTagging(checkpoint_path=None, device='cpu')

# Process audio and extract tags
def process_audio(audio_path):
    audio, _ = librosa.load(audio_path, sr=sr, mono=True)
    segment_length = int(clip_duration * sr)
    hop_length = int((clip_duration - overlap) * sr)

    tags_list = []
    for start_idx in range(0, len(audio) - segment_length + 1, hop_length):
        segment = audio[start_idx : start_idx + segment_length][None, :]
        clipwise_output, _ = at.inference(segment)
        tags = [(labels[i], score) for i, score in enumerate(clipwise_output[0]) if score > threshold]
        tags_list.append(tags)
    
    return tags_list

def extract_nouns_and_verbs(captions_data):
    connected_words = {}
    for key, captions in list(captions_data.items())[:num_clips]:
        all_captions = captions.get("audio_captions", []) 
        connected_words[key] = {"nouns": [], "verbs": []}
        
        for caption in all_captions:
            doc = nlp(caption)
            for token in doc:
                if token.pos_ == "NOUN":
                    connected_words[key]["nouns"].append(token.text)
                elif token.pos_ == "VERB":
                    connected_words[key]["verbs"].append(token.text)
    return connected_words

# Similarity between audio tags and text captions
def compute_similarity(audio_tags, caption_words):
    if not caption_words:
        return [(tag, conf, conf * weight_audio_conf) for tag, conf in audio_tags]  # Reduced confidence without text
    
    if not audio_tags:
        return [] 

    audio_labels = [tag for tag, _ in audio_tags]
    audio_embeddings = sbert_model.encode(audio_labels, batch_size=32, show_progress_bar=False) 
    text_embeddings = sbert_model.encode(caption_words, batch_size=32, show_progress_bar=False)

    if text_embeddings.shape[0] == 0:  # If no valid text embeddings
        return [(tag, conf, conf * weight_audio_conf) for tag, conf in audio_tags]

    similarity_matrix = util.cos_sim(audio_embeddings, text_embeddings).numpy()
    
    confidence_scores = []
    for i, (tag, conf) in enumerate(audio_tags):
        max_sim = np.max(similarity_matrix[i]) if similarity_matrix.shape[1] > 0 else 0
        final_conf = weight_audio_conf * conf + weight_text_sim * max_sim
        confidence_scores.append((tag, conf, final_conf))  # Store both original and adjusted confidence
    
    return confidence_scores

if __name__ == "__main__":
    with open(json_file_path, 'r') as f:
        captions_data = json.load(f)

    all_tags = []
    processed_clips = 0
    for file_name in sorted(os.listdir(audio_folder)):
        if processed_clips >= num_clips:
            break
        if file_name.endswith('.wav'):
            audio_path = os.path.join(audio_folder, file_name)
            tags = process_audio(audio_path)
            all_tags.append(tags)
            processed_clips += 1

    connected_words = extract_nouns_and_verbs(captions_data)

    # clip processing
    for i, clip_tags in enumerate(all_tags):
        key = list(connected_words.keys())[i] if i < len(connected_words) else None
        if key:
            caption_words = connected_words[key]['nouns'] + connected_words[key]['verbs']

            # Print caption words only once
            print(f"\nAudio Clip {i + 1} - Caption Words: {', '.join(caption_words)}")
            print("-" * 60)
            print(f"{'Segment':<10} {'Tag':<30} {'Audio Conf.':<15} {'Final Conf.':<15}")
            print("-" * 60)

            # Process each segment in the clip and print the results neatly
            for j, segment_tags in enumerate(clip_tags):
                final_scores = compute_similarity(segment_tags, caption_words)

                if final_scores:  # Only add results if there are valid confidence scores
                    for tag, audio_conf, final_conf in final_scores:
                        print(f"{j + 1:<10} {tag:<30} {audio_conf:.2f}         {final_conf:.2f}")

            print("-" * 60)

Version 5 - 
Processing top 3 tags in audio rather than only ones with threshold >0.5. Also adding other captions in addition to just audio. (AV, gptcaps etc). Added phrase instead of words for captions
import os
import librosa
import numpy as np
import json
import spacy
from panns_inference import AudioTagging, labels
from sentence_transformers import SentenceTransformer, util

audio_folder = './data/test_audio_output'
json_file_path = './data/test_captions.json'

# Parameters
num_clips = 5  # Number of audio clips to process
sr = 32000  # Sample rate
clip_duration = 5  # Audio segment length
overlap = 3  # Overlap in audio (s)
weight_audio_conf = 0.8  # Audio confidence weight
weight_text_sim = 0.2  # Text similarity weight
top_n_tags = 3  # Take the top 3 tags regardless of confidence

nlp = spacy.load('en_core_web_sm')
sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
at = AudioTagging(checkpoint_path=None, device='cpu')

# Process audio and extract tags
def process_audio(audio_path):
    audio, _ = librosa.load(audio_path, sr=sr, mono=True)
    segment_length = int(clip_duration * sr)
    hop_length = int((clip_duration - overlap) * sr)

    tags_list = []
    for start_idx in range(0, len(audio) - segment_length + 1, hop_length):
        segment = audio[start_idx : start_idx + segment_length][None, :]
        clipwise_output, _ = at.inference(segment)

        # Get top 3 tags instead of using a threshold
        sorted_indices = np.argsort(clipwise_output[0])[::-1][:top_n_tags]
        tags = [(labels[i], clipwise_output[0][i]) for i in sorted_indices]
        tags_list.append(tags)

    return tags_list

def extract_noun_verb_phrases(captions_data):
    connected_phrases = {}

    for key, captions in list(captions_data.items())[:num_clips]:
        # Combine all caption types (audio, visual, audio-visual, GPT-AV)
        all_captions = captions.get("audio_captions", []) + captions.get("visual_captions", []) + captions.get("audio_visual_captions", []) + captions.get("GPT_AV_captions", [])

        connected_phrases[key] = {"noun_verb_pairs": [], "compound_nouns": []}

        for caption in all_captions:
            doc = nlp(caption)
            noun_verb_pairs = set()
            compound_nouns = set()
            
            for token in doc:
                # 1️⃣ Extract Subject-Verb Pairs (e.g., "man speaking", "car driving")
                if token.pos_ == "VERB":
                    for child in token.children:
                        if child.dep_ in ("nsubj", "nsubjpass"):  # Subject of the verb
                            phrase = f"{child.text} {token.text}"  # Example: "man speaking"
                            noun_verb_pairs.add(phrase)

                # 2️⃣ Extract Compound Nouns (e.g., "sports car", "big crowd")
                if token.dep_ == "compound" and token.head.pos_ == "NOUN":
                    phrase = f"{token.text} {token.head.text}"  # Example: "sports car"
                    compound_nouns.add(phrase)

            connected_phrases[key]["noun_verb_pairs"] = list(noun_verb_pairs)
            connected_phrases[key]["compound_nouns"] = list(compound_nouns)

    return connected_phrases

# Similarity between audio tags and text captions
def compute_similarity(audio_tags, caption_words):
    if not caption_words:
        return [(tag, conf, conf * weight_audio_conf) for tag, conf in audio_tags]  # Reduced confidence without text
    
    if not audio_tags:
        return [] 

    audio_labels = [tag for tag, _ in audio_tags]
    audio_embeddings = sbert_model.encode(audio_labels, batch_size=32, show_progress_bar=False) 
    text_embeddings = sbert_model.encode(caption_words, batch_size=32, show_progress_bar=False)

    if text_embeddings.shape[0] == 0:  # If no valid text embeddings
        return [(tag, conf, conf * weight_audio_conf) for tag, conf in audio_tags]

    similarity_matrix = util.cos_sim(audio_embeddings, text_embeddings).numpy()
    
    confidence_scores = []
    for i, (tag, conf) in enumerate(audio_tags):
        max_sim = np.max(similarity_matrix[i]) if similarity_matrix.shape[1] > 0 else 0
        final_conf = weight_audio_conf * conf + weight_text_sim * max_sim
        confidence_scores.append((tag, conf, final_conf))  # Store both original and adjusted confidence
    
    return confidence_scores

if __name__ == "__main__":
    with open(json_file_path, 'r') as f:
        captions_data = json.load(f)

    all_tags = []
    processed_clips = 0
    for file_name in sorted(os.listdir(audio_folder)):
        if processed_clips >= num_clips:
            break
        if file_name.endswith('.wav'):
            audio_path = os.path.join(audio_folder, file_name)
            tags = process_audio(audio_path)
            all_tags.append(tags)
            processed_clips += 1

    connected_phrases = extract_noun_verb_phrases(captions_data)

    # Clip processing
    for i, clip_tags in enumerate(all_tags):
        key = list(connected_phrases.keys())[i] if i < len(connected_phrases) else None
        if key:
            caption_words = connected_phrases[key]["noun_verb_pairs"] + connected_phrases[key]["compound_nouns"]

            # Print caption words only once at the top for the clip
            print(f"\nAudio Clip {i + 1} - Caption Words: {', '.join(caption_words)}")
            print("-" * 60)
            print(f"{'Segment':<10} {'Tag':<30} {'Audio Conf.':<15} {'Final Conf.':<15}")
            print("-" * 60)

            # Process each segment in the clip and print the results neatly
            for j, segment_tags in enumerate(clip_tags):
                final_scores = compute_similarity(segment_tags, caption_words)

                if final_scores:  # Only add results if there are valid confidence scores
                    for tag, audio_conf, final_conf in final_scores:
                        print(f"{j + 1:<10} {tag:<30} {audio_conf:.2f}         {final_conf:.2f}")

            print("-" * 60)  # Separator between clips


Version 6 -
Finding best cosine similarities rather than all.

import os
import librosa
import numpy as np
import json
import spacy
from panns_inference import AudioTagging, labels
from sentence_transformers import SentenceTransformer, util

audio_folder = './data/test_audio_output'
json_file_path = './data/test_captions.json'

# Parameters
num_clips = 5  # Number of audio clips to process
sr = 32000  # Sample rate
clip_duration = 5  # Audio segment length
overlap = 2  # Overlap in audio (s)
weight_audio_conf = 0.5  # Audio confidence weight
weight_text_sim = 0.5  # Text similarity weight
top_n_tags = 3  # Take the top 3 tags regardless of confidence

nlp = spacy.load('en_core_web_sm')
sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
at = AudioTagging(checkpoint_path=None, device='cpu')

# Process audio and extract tags
def process_audio(audio_path):
    audio, _ = librosa.load(audio_path, sr=sr, mono=True)
    segment_length = int(clip_duration * sr)
    hop_length = int((clip_duration - overlap) * sr)

    tags_list = []
    for start_idx in range(0, len(audio) - segment_length + 1, hop_length):
        segment = audio[start_idx : start_idx + segment_length][None, :]
        clipwise_output, _ = at.inference(segment)

        # Get top 3 tags instead of using a threshold
        sorted_indices = np.argsort(clipwise_output[0])[::-1][:top_n_tags]
        tags = [(labels[i], clipwise_output[0][i]) for i in sorted_indices]
        tags_list.append(tags)

    return tags_list

def extract_noun_verb_phrases(captions_data):
    connected_phrases = {}

    for key, captions in list(captions_data.items())[:num_clips]:
        all_captions = (
            captions.get("audio_captions", []) 
            + captions.get("visual_captions", []) 
            + captions.get("audio_visual_captions", []) 
            + captions.get("GPT_AV_captions", [])
        )

        connected_phrases[key] = {"noun_phrases": [], "verb_phrases": [], "individual_words": []}

        for caption in all_captions:
            doc = nlp(caption)

            noun_phrases = set()
            verb_phrases = set()
            individual_words = set()

            for chunk in doc.noun_chunks:
                noun_phrases.add(chunk.text)

            for token in doc:
                if token.pos_ == "VERB":
                    phrase = token.text
                    obj = [child.text for child in token.children if child.dep_ in ("dobj", "prep", "pobj")]
                    if obj:
                        phrase += " " + " ".join(obj)
                    verb_phrases.add(phrase)

                if token.pos_ in {"NOUN", "VERB"}:
                    individual_words.add(token.text)  # Store standalone nouns/verbs

            connected_phrases[key]["noun_phrases"] = list(noun_phrases)
            connected_phrases[key]["verb_phrases"] = list(verb_phrases)
            connected_phrases[key]["individual_words"] = list(individual_words)

    return connected_phrases


def compute_similarity(audio_tags, caption_words):
    if not caption_words:
        return [(tag, conf, conf * weight_audio_conf) for tag, conf in audio_tags]  

    if not audio_tags:
        return []

    audio_labels = [tag for tag, _ in audio_tags]
    audio_embeddings = sbert_model.encode(audio_labels, batch_size=32, show_progress_bar=False)
    text_embeddings = sbert_model.encode(caption_words, batch_size=32, show_progress_bar=False)

    if text_embeddings.shape[0] == 0:  
        return [(tag, conf, conf * weight_audio_conf) for tag, conf in audio_tags]

    similarity_matrix = util.cos_sim(audio_embeddings, text_embeddings).numpy()

    confidence_scores = []
    for i, (tag, conf) in enumerate(audio_tags):
        if similarity_matrix.shape[1] > 0:
            max_sim = np.max(similarity_matrix[i]) 
        else:
            max_sim = 0  

        final_conf = weight_audio_conf * conf + weight_text_sim * max_sim
        confidence_scores.append((tag, conf, final_conf))

    return confidence_scores


if __name__ == "__main__":
    with open(json_file_path, 'r') as f:
        captions_data = json.load(f)

    all_tags = []
    processed_clips = 0
    for file_name in sorted(os.listdir(audio_folder)):
        if processed_clips >= num_clips:
            break
        if file_name.endswith('.wav'):
            audio_path = os.path.join(audio_folder, file_name)
            tags = process_audio(audio_path)
            all_tags.append(tags)
            processed_clips += 1

    connected_phrases = extract_noun_verb_phrases(captions_data)

    # Clip processing
    for i, clip_tags in enumerate(all_tags):
        key = list(connected_phrases.keys())[i] if i < len(connected_phrases) else None
        if key:
            caption_words = (
                connected_phrases[key]["noun_phrases"] 
                + connected_phrases[key]["verb_phrases"] 
                + connected_phrases[key]["individual_words"]
            )


            print(f"\nAudio Clip {i + 1} - Caption Words: {', '.join(caption_words)}")
            print("-" * 60)
            print(f"{'Segment':<10} {'Tag':<30} {'Audio Conf.':<15} {'Final Conf.':<15}")
            print("-" * 60)

            for j, segment_tags in enumerate(clip_tags):
                final_scores = compute_similarity(segment_tags, caption_words)

                if final_scores:
                    for tag, audio_conf, final_conf in final_scores:
                        print(f"{j + 1:<10} {tag:<30} {audio_conf:.2f}         {final_conf:.2f}")

            print("-" * 60)


Version 7:
Using only audio captions, finding unique tags in audio with max confidence from some segment. Using subject-object-verb combination for extracting captions.

import os
import librosa
import numpy as np
import json
import spacy
from panns_inference import AudioTagging, labels
from sentence_transformers import SentenceTransformer, util
from subject_verb_object import *  # Assuming this is for extracting SVO triples

audio_folder = './data/test_audio_output'
json_file_path = './data/test_captions.json'

# Parameters
num_clips = 5  # Number of audio clips to process
sr = 32000  # Sample rate
clip_duration = 5  # Audio segment length
overlap = 2  # Overlap in audio (s)
weight_audio_conf = 0.5  # Audio confidence weight
weight_text_sim = 0.5  # Text similarity weight
top_n_tags = 3  # Take the top 3 tags regardless of confidence

nlp = spacy.load('en_core_web_sm')
sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')
at = AudioTagging(checkpoint_path=None, device='cpu')

# Extract SVO (Subject-Verb-Object) triples and related phrases from audio captions
def extract_svo_from_captions(captions_data):
    nlp_svo = spacy.load('en_core_web_trf')  # Using transformer model for better accuracy

    connected_phrases = {}

    for key, captions in list(captions_data.items())[:num_clips]:
        audio_captions = captions.get("audio_captions", [])  # Only use audio captions

        connected_phrases[key] = {"svo_triples": [], "noun_phrases": [], "individual_words": []}

        for caption in audio_captions:
            doc = nlp_svo(caption)
            svos = findSVAOs(doc)  # Extract Subject-Verb-Object triples

            noun_phrases = set()
            individual_words = set()

            for chunk in doc.noun_chunks:
                noun_phrases.add(chunk.text)

            for token in doc:
                if token.pos_ in {"NOUN", "VERB"}:
                    individual_words.add(token.text)

            connected_phrases[key]["svo_triples"].extend(svos)
            connected_phrases[key]["noun_phrases"] = list(noun_phrases)
            connected_phrases[key]["individual_words"] = list(individual_words)

    return connected_phrases

# Process audio and extract tags
def process_audio(audio_path):
    audio, _ = librosa.load(audio_path, sr=sr, mono=True)
    segment_length = int(clip_duration * sr)
    hop_length = int((clip_duration - overlap) * sr)

    tags_list = []
    clipwise_confidences = {}

    for start_idx in range(0, len(audio) - segment_length + 1, hop_length):
        segment = audio[start_idx : start_idx + segment_length][None, :]
        clipwise_output, _ = at.inference(segment)

        sorted_indices = np.argsort(clipwise_output[0])[::-1][:top_n_tags]
        tags = [(labels[i], clipwise_output[0][i]) for i in sorted_indices]

        # Store segment tags but do not print them
        tags_list.append(tags)

        # Track max confidence for each tag over all segments
        for label, confidence in tags:
            if label not in clipwise_confidences or confidence > clipwise_confidences[label]:
                clipwise_confidences[label] = confidence

    # Convert dictionary to sorted list based on confidence
    clipwise_tags = sorted(clipwise_confidences.items(), key=lambda x: x[1], reverse=True)

    return tags_list, clipwise_tags


# Compute similarity between audio tags and caption words
def compute_similarity(audio_tags, caption_words):
    if not caption_words:
        return [(tag, conf, conf * weight_audio_conf) for tag, conf in audio_tags]  

    if not audio_tags:
        return []

    audio_labels = [tag for tag, _ in audio_tags]
    audio_embeddings = sbert_model.encode(audio_labels, batch_size=32, show_progress_bar=False)
    text_embeddings = sbert_model.encode(caption_words, batch_size=32, show_progress_bar=False)

    if text_embeddings.shape[0] == 0:  
        return [(tag, conf, conf * weight_audio_conf) for tag, conf in audio_tags]

    similarity_matrix = util.cos_sim(audio_embeddings, text_embeddings).numpy()

    confidence_scores = []
    for i, (tag, conf) in enumerate(audio_tags):
        max_sim = np.max(similarity_matrix[i]) if similarity_matrix.shape[1] > 0 else 0
        boosted_conf = weight_audio_conf * conf + weight_text_sim * max_sim
        confidence_scores.append((tag, conf, boosted_conf))

    return confidence_scores


if __name__ == "__main__":
    with open(json_file_path, 'r') as f:
        captions_data = json.load(f)

    connected_phrases = extract_svo_from_captions(captions_data)  # Extract SVO triples and phrases

    all_tags = []
    processed_clips = 0

    # Process the audio files
    for file_name in sorted(os.listdir(audio_folder)):
        if processed_clips >= num_clips:
            break
        if file_name.endswith('.wav'):
            audio_path = os.path.join(audio_folder, file_name)
            segment_tags, clipwise_tags = process_audio(audio_path)
            all_tags.append((segment_tags, clipwise_tags))  # Store both segment-wise and clip-level tags
            processed_clips += 1

    # Clip processing
    for i, (segment_tags, clipwise_tags) in enumerate(all_tags):
        # Extract the number from the file name (e.g., '1.wav' -> '1')
        file_name = sorted(os.listdir(audio_folder))[i]  # Get the current file name
        file_number = file_name.split('.')[0]  # Remove the '.wav' extension and get the number
        
        key = list(connected_phrases.keys())[i] if i < len(connected_phrases) else None
        if key:
            caption_words = (
                connected_phrases[key]["noun_phrases"] 
                + connected_phrases[key]["individual_words"]
            )

            print(f"\n*")
            print(f"SVO Triples: {connected_phrases[key]['svo_triples']}")
            print(f"Extracted Words: {', '.join(caption_words)}")
            print(f"*")
            print("-" * 80)
            print(f"{'Tag':<30} {'Audio Conf.':<15} {'Boosted Conf.':<15}")
            print("-" * 80)

            # Process each segment
            for tag, audio_conf, boosted_conf in compute_similarity(clipwise_tags, caption_words):
                print(f"{tag:<30} {audio_conf:.2f}         {boosted_conf:.2f}")

            print("-" * 80)


Version 8:
Trying to process captions to match ontology:




TO TEST:
Different evaluation metrics (different distances/similarities (euclidian, manhattan))
Preprocess - normalize, denoise audio; lemmatization/stemming (root form eg running -> run)
Finetuning - PANNS, SentenceTransformer, domain-specific or general.
Different models BERT, RoBERTa, GPT
Expanding text matching, contextual similarity
Cross-modal attention, instead of embeddings, attention mechanisms - relevance
Dynamic thresholding
Hyperparameter testing (grid search, random search, bayesian optimization) for best performance

